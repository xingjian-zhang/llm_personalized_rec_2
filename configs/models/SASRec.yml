n_layers: 2                     # (int) The number of transformer layers in transformer encoder.
n_heads: 2                      # (int) The number of attention heads for multi-head attention layer.
hidden_size: 64                 # (int) The number of features in the hidden state.
inner_size: 256                 # (int) The inner hidden size in feed-forward layer.
hidden_dropout_prob: 0.3        # (float) The probability of an element to be zeroed.
attn_dropout_prob: 0.3          # (float) The probability of an attention score to be zeroed.
hidden_act: 'gelu'              # (str) The activation function in feed-forward layer.

# Training and evaluation config
epochs: 100
train_batch_size: 4096
eval_batch_size: 4096
train_neg_sample_args: ~
eval_args:
    group_by: user
    order: TO
    split: {'LS': 'valid_and_test'}
    mode: labeled
metrics: ['RMSE', 'MAE', 'AUC']
valid_metric: AUC
